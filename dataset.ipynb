{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the dataset with traditional method.\n",
    "\n",
    "Input:\n",
    "    file path to the dataset\n",
    "\n",
    "Return:\n",
    "    (data, label) as np.array\n",
    "'''\n",
    "import numpy as np\n",
    "def get_dataset(filePath):\n",
    "    allCQT = None\n",
    "    allLabel = None\n",
    "    for i, song in enumerate(filePath):\n",
    "        data = np.load(song)\n",
    "        if (i == 0):\n",
    "            allCQT = data['cqt']\n",
    "            allLabel = data['label']\n",
    "        else:      \n",
    "            allCQT = np.append(allCQT, data['cqt'], axis=0)\n",
    "            allLabel = np.append(allLabel, data['label'], axis=0)\n",
    "    \n",
    "    return allCQT, allLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01522d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Partition the dataset of np.array.\n",
    "\n",
    "Input:\n",
    "    data and label as np.array\n",
    "\n",
    "Return:\n",
    "    partitioned dataset as np.array\n",
    "'''\n",
    "import numpy as np\n",
    "def partitionDataset(data, label, val_ratio=0.1, test_ratio=0.1, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    size = data.shape[0]\n",
    "    indices = np.random.permutation(size)\n",
    "    training_idx, val_idx, test_idx = indices[:int((1-val_ratio-test_ratio)*size)], indices[int((1-val_ratio-test_ratio)*size):int((1-test_ratio)*size)], indices[int((1-test_ratio)*size):]\n",
    "    x_train, x_val, x_test = data[training_idx,:], data[val_idx,:], data[test_idx,:]\n",
    "    y_train, y_val, y_test = label[training_idx], label[val_idx], label[test_idx]\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25091f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch wrapper of beatles dataset.\n",
    "\n",
    "Initialization:\n",
    "    file path to the dataset\n",
    "\n",
    "Return:\n",
    "    PyTorch dataset\n",
    "'''\n",
    "from torch.utils.data import Dataset\n",
    "class BeatlesDataset(Dataset):\n",
    "    def __init__(self, filePath, output_size=25):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")     \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        allCQT, allLabel = get_dataset(filePath)\n",
    "        self.allCQT = allCQT\n",
    "        self.allLabel = allLabel\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.allCQT)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        x = torch.from_numpy(self.allCQT[idx, :]).to(self.device)\n",
    "        y = torch.zeros(self.output_size).to(self.device)\n",
    "        y[self.allLabel[idx]] = 1\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split the dataset and create respective data_loader.\n",
    "\n",
    "Input:\n",
    "    Whole dataset\n",
    "\n",
    "Return:\n",
    "    3 dataloader\n",
    "'''\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "def datasetSplit(dataset, val_ratio=0.1, test_ratio=0.1, batch_size=512, shuffle=True, drop_last=True):\n",
    "    # calculate the size of portion\n",
    "    val_size = int(val_ratio * len(dataset))\n",
    "    test_size = int(test_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size - test_size\n",
    "    \n",
    "    # split and create dataloader\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usecase\n",
    "# beatles_dataset = BeatlesDataset(npzs_8th)\n",
    "# train_loader, val_loader, test_loader = datasetSplit(beatles_dataset, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
