{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.utility import *\n",
    "from ipynb.fs.full.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4b4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f746960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "input_size = 138\n",
    "hidden_size = 200\n",
    "output_size = 25\n",
    "num_epochs = 100\n",
    "BATCH_SIZE = 512\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39e9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e148475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1550a1d9e690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772149c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_dataset = BeatlesDataset(npzs_8th)\n",
    "train_loader, val_loader, test_loader = datasetSplit(beatles_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640208c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7c205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f7828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f57d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 123])\n",
      "torch.Size([512, 25])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722ba884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/741], Loss: 2.8548\n",
      "Epoch [1/100], Step [200/741], Loss: 2.7656\n",
      "Epoch [1/100], Step [300/741], Loss: 2.7196\n",
      "Epoch [1/100], Step [400/741], Loss: 2.7537\n",
      "Epoch [1/100], Step [500/741], Loss: 2.6810\n",
      "Epoch [1/100], Step [600/741], Loss: 2.6995\n",
      "Epoch [1/100], Step [700/741], Loss: 2.7437\n",
      "Epoch [2/100], Step [100/741], Loss: 2.7087\n",
      "Epoch [2/100], Step [200/741], Loss: 2.7178\n",
      "Epoch [2/100], Step [300/741], Loss: 2.6999\n",
      "Epoch [2/100], Step [400/741], Loss: 2.6683\n",
      "Epoch [2/100], Step [500/741], Loss: 2.6342\n",
      "Epoch [2/100], Step [600/741], Loss: 2.5534\n",
      "Epoch [2/100], Step [700/741], Loss: 2.6284\n",
      "Epoch [3/100], Step [100/741], Loss: 2.6842\n",
      "Epoch [3/100], Step [200/741], Loss: 2.4892\n",
      "Epoch [3/100], Step [300/741], Loss: 2.5360\n",
      "Epoch [3/100], Step [400/741], Loss: 2.5565\n",
      "Epoch [3/100], Step [500/741], Loss: 2.5429\n",
      "Epoch [3/100], Step [600/741], Loss: 2.6225\n",
      "Epoch [3/100], Step [700/741], Loss: 2.5806\n",
      "Epoch [4/100], Step [100/741], Loss: 2.6125\n",
      "Epoch [4/100], Step [200/741], Loss: 2.5270\n",
      "Epoch [4/100], Step [300/741], Loss: 2.5422\n",
      "Epoch [4/100], Step [400/741], Loss: 2.4934\n",
      "Epoch [4/100], Step [500/741], Loss: 2.5668\n",
      "Epoch [4/100], Step [600/741], Loss: 2.5819\n",
      "Epoch [4/100], Step [700/741], Loss: 2.5499\n",
      "Epoch [5/100], Step [100/741], Loss: 2.5315\n",
      "Epoch [5/100], Step [200/741], Loss: 2.4719\n",
      "Epoch [5/100], Step [300/741], Loss: 2.5008\n",
      "Epoch [5/100], Step [400/741], Loss: 2.4779\n",
      "Epoch [5/100], Step [500/741], Loss: 2.5066\n",
      "Epoch [5/100], Step [600/741], Loss: 2.5522\n",
      "Epoch [5/100], Step [700/741], Loss: 2.5310\n",
      "Epoch [6/100], Step [100/741], Loss: 2.3899\n",
      "Epoch [6/100], Step [200/741], Loss: 2.5512\n",
      "Epoch [6/100], Step [300/741], Loss: 2.4310\n",
      "Epoch [6/100], Step [400/741], Loss: 2.5319\n",
      "Epoch [6/100], Step [500/741], Loss: 2.4687\n",
      "Epoch [6/100], Step [600/741], Loss: 2.4621\n",
      "Epoch [6/100], Step [700/741], Loss: 2.4537\n",
      "Epoch [7/100], Step [100/741], Loss: 2.4755\n",
      "Epoch [7/100], Step [200/741], Loss: 2.4488\n",
      "Epoch [7/100], Step [300/741], Loss: 2.4010\n",
      "Epoch [7/100], Step [400/741], Loss: 2.4815\n",
      "Epoch [7/100], Step [500/741], Loss: 2.3549\n",
      "Epoch [7/100], Step [600/741], Loss: 2.3655\n",
      "Epoch [7/100], Step [700/741], Loss: 2.4574\n",
      "Epoch [8/100], Step [100/741], Loss: 2.3974\n",
      "Epoch [8/100], Step [200/741], Loss: 2.3817\n",
      "Epoch [8/100], Step [300/741], Loss: 2.3478\n",
      "Epoch [8/100], Step [400/741], Loss: 2.3737\n",
      "Epoch [8/100], Step [500/741], Loss: 2.3237\n",
      "Epoch [8/100], Step [600/741], Loss: 2.4670\n",
      "Epoch [8/100], Step [700/741], Loss: 2.3685\n",
      "Epoch [9/100], Step [100/741], Loss: 2.4193\n",
      "Epoch [9/100], Step [200/741], Loss: 2.4193\n",
      "Epoch [9/100], Step [300/741], Loss: 2.3633\n",
      "Epoch [9/100], Step [400/741], Loss: 2.3252\n",
      "Epoch [9/100], Step [500/741], Loss: 2.4345\n",
      "Epoch [9/100], Step [600/741], Loss: 2.4657\n",
      "Epoch [9/100], Step [700/741], Loss: 2.4721\n",
      "Epoch [10/100], Step [100/741], Loss: 2.4612\n",
      "Epoch [10/100], Step [200/741], Loss: 2.3050\n",
      "Epoch [10/100], Step [300/741], Loss: 2.3914\n",
      "Epoch [10/100], Step [400/741], Loss: 2.4115\n",
      "Epoch [10/100], Step [500/741], Loss: 2.3477\n",
      "Epoch [10/100], Step [600/741], Loss: 2.4205\n",
      "Epoch [10/100], Step [700/741], Loss: 2.3533\n",
      "Epoch [11/100], Step [100/741], Loss: 2.4054\n",
      "Epoch [11/100], Step [200/741], Loss: 2.3946\n",
      "Epoch [11/100], Step [300/741], Loss: 2.3890\n",
      "Epoch [11/100], Step [400/741], Loss: 2.3205\n",
      "Epoch [11/100], Step [500/741], Loss: 2.4122\n",
      "Epoch [11/100], Step [600/741], Loss: 2.3438\n",
      "Epoch [11/100], Step [700/741], Loss: 2.4082\n",
      "Epoch [12/100], Step [100/741], Loss: 2.2821\n",
      "Epoch [12/100], Step [200/741], Loss: 2.4468\n",
      "Epoch [12/100], Step [300/741], Loss: 2.3563\n",
      "Epoch [12/100], Step [400/741], Loss: 2.3705\n",
      "Epoch [12/100], Step [500/741], Loss: 2.3812\n",
      "Epoch [12/100], Step [600/741], Loss: 2.3051\n",
      "Epoch [12/100], Step [700/741], Loss: 2.2706\n",
      "Epoch [13/100], Step [100/741], Loss: 2.3295\n",
      "Epoch [13/100], Step [200/741], Loss: 2.3378\n",
      "Epoch [13/100], Step [300/741], Loss: 2.4062\n",
      "Epoch [13/100], Step [400/741], Loss: 2.4180\n",
      "Epoch [13/100], Step [500/741], Loss: 2.4336\n",
      "Epoch [13/100], Step [600/741], Loss: 2.3665\n",
      "Epoch [13/100], Step [700/741], Loss: 2.4083\n",
      "Epoch [14/100], Step [100/741], Loss: 2.3344\n",
      "Epoch [14/100], Step [200/741], Loss: 2.4090\n",
      "Epoch [14/100], Step [300/741], Loss: 2.3673\n",
      "Epoch [14/100], Step [400/741], Loss: 2.4037\n",
      "Epoch [14/100], Step [500/741], Loss: 2.3249\n",
      "Epoch [14/100], Step [600/741], Loss: 2.3151\n",
      "Epoch [14/100], Step [700/741], Loss: 2.3928\n",
      "Epoch [15/100], Step [100/741], Loss: 2.3541\n",
      "Epoch [15/100], Step [200/741], Loss: 2.2796\n",
      "Epoch [15/100], Step [300/741], Loss: 2.3363\n",
      "Epoch [15/100], Step [400/741], Loss: 2.3328\n",
      "Epoch [15/100], Step [500/741], Loss: 2.3615\n",
      "Epoch [15/100], Step [600/741], Loss: 2.3899\n",
      "Epoch [15/100], Step [700/741], Loss: 2.3258\n",
      "Epoch [16/100], Step [100/741], Loss: 2.2490\n",
      "Epoch [16/100], Step [200/741], Loss: 2.3468\n",
      "Epoch [16/100], Step [300/741], Loss: 2.3486\n",
      "Epoch [16/100], Step [400/741], Loss: 2.3569\n",
      "Epoch [16/100], Step [500/741], Loss: 2.3744\n",
      "Epoch [16/100], Step [600/741], Loss: 2.3807\n",
      "Epoch [16/100], Step [700/741], Loss: 2.2953\n",
      "Epoch [17/100], Step [100/741], Loss: 2.2814\n",
      "Epoch [17/100], Step [200/741], Loss: 2.3301\n",
      "Epoch [17/100], Step [300/741], Loss: 2.3335\n",
      "Epoch [17/100], Step [400/741], Loss: 2.3687\n",
      "Epoch [17/100], Step [500/741], Loss: 2.4128\n",
      "Epoch [17/100], Step [600/741], Loss: 2.3709\n",
      "Epoch [17/100], Step [700/741], Loss: 2.3293\n",
      "Epoch [18/100], Step [100/741], Loss: 2.3410\n",
      "Epoch [18/100], Step [200/741], Loss: 2.3232\n",
      "Epoch [18/100], Step [300/741], Loss: 2.3323\n",
      "Epoch [18/100], Step [400/741], Loss: 2.3793\n",
      "Epoch [18/100], Step [500/741], Loss: 2.3683\n",
      "Epoch [18/100], Step [600/741], Loss: 2.3734\n",
      "Epoch [18/100], Step [700/741], Loss: 2.3647\n",
      "Epoch [19/100], Step [100/741], Loss: 2.3124\n",
      "Epoch [19/100], Step [200/741], Loss: 2.3935\n",
      "Epoch [19/100], Step [300/741], Loss: 2.2862\n",
      "Epoch [19/100], Step [400/741], Loss: 2.3673\n",
      "Epoch [19/100], Step [500/741], Loss: 2.3186\n",
      "Epoch [19/100], Step [600/741], Loss: 2.3055\n",
      "Epoch [19/100], Step [700/741], Loss: 2.3563\n",
      "Epoch [20/100], Step [100/741], Loss: 2.3833\n",
      "Epoch [20/100], Step [200/741], Loss: 2.2763\n",
      "Epoch [20/100], Step [300/741], Loss: 2.3059\n",
      "Epoch [20/100], Step [400/741], Loss: 2.2849\n",
      "Epoch [20/100], Step [500/741], Loss: 2.2116\n",
      "Epoch [20/100], Step [600/741], Loss: 2.2917\n",
      "Epoch [20/100], Step [700/741], Loss: 2.2332\n",
      "Epoch [21/100], Step [100/741], Loss: 2.2994\n",
      "Epoch [21/100], Step [200/741], Loss: 2.2555\n",
      "Epoch [21/100], Step [300/741], Loss: 2.3228\n",
      "Epoch [21/100], Step [400/741], Loss: 2.3081\n",
      "Epoch [21/100], Step [500/741], Loss: 2.3121\n",
      "Epoch [21/100], Step [600/741], Loss: 2.2651\n",
      "Epoch [21/100], Step [700/741], Loss: 2.3449\n",
      "Epoch [22/100], Step [100/741], Loss: 2.2742\n",
      "Epoch [22/100], Step [200/741], Loss: 2.3065\n",
      "Epoch [22/100], Step [300/741], Loss: 2.3143\n",
      "Epoch [22/100], Step [400/741], Loss: 2.3110\n",
      "Epoch [22/100], Step [500/741], Loss: 2.3317\n",
      "Epoch [22/100], Step [600/741], Loss: 2.3280\n",
      "Epoch [22/100], Step [700/741], Loss: 2.3009\n",
      "Epoch [23/100], Step [100/741], Loss: 2.3691\n",
      "Epoch [23/100], Step [200/741], Loss: 2.3224\n",
      "Epoch [23/100], Step [300/741], Loss: 2.3207\n",
      "Epoch [23/100], Step [400/741], Loss: 2.3260\n",
      "Epoch [23/100], Step [500/741], Loss: 2.3602\n",
      "Epoch [23/100], Step [600/741], Loss: 2.3822\n",
      "Epoch [23/100], Step [700/741], Loss: 2.3808\n",
      "Epoch [24/100], Step [100/741], Loss: 2.4071\n",
      "Epoch [24/100], Step [200/741], Loss: 2.3549\n",
      "Epoch [24/100], Step [300/741], Loss: 2.1973\n",
      "Epoch [24/100], Step [400/741], Loss: 2.3139\n",
      "Epoch [24/100], Step [500/741], Loss: 2.3542\n",
      "Epoch [24/100], Step [600/741], Loss: 2.3369\n",
      "Epoch [24/100], Step [700/741], Loss: 2.3560\n",
      "Epoch [25/100], Step [100/741], Loss: 2.3122\n",
      "Epoch [25/100], Step [200/741], Loss: 2.3090\n",
      "Epoch [25/100], Step [300/741], Loss: 2.2799\n",
      "Epoch [25/100], Step [400/741], Loss: 2.2195\n",
      "Epoch [25/100], Step [500/741], Loss: 2.3445\n",
      "Epoch [25/100], Step [600/741], Loss: 2.3406\n",
      "Epoch [25/100], Step [700/741], Loss: 2.3644\n",
      "Epoch [26/100], Step [100/741], Loss: 2.3399\n",
      "Epoch [26/100], Step [200/741], Loss: 2.2416\n",
      "Epoch [26/100], Step [300/741], Loss: 2.3278\n",
      "Epoch [26/100], Step [400/741], Loss: 2.2767\n",
      "Epoch [26/100], Step [500/741], Loss: 2.2556\n",
      "Epoch [26/100], Step [600/741], Loss: 2.3468\n",
      "Epoch [26/100], Step [700/741], Loss: 2.3430\n",
      "Epoch [27/100], Step [100/741], Loss: 2.2746\n",
      "Epoch [27/100], Step [200/741], Loss: 2.2548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Step [300/741], Loss: 2.2418\n",
      "Epoch [27/100], Step [400/741], Loss: 2.3604\n",
      "Epoch [27/100], Step [500/741], Loss: 2.3303\n",
      "Epoch [27/100], Step [600/741], Loss: 2.3137\n",
      "Epoch [27/100], Step [700/741], Loss: 2.2952\n",
      "Epoch [28/100], Step [100/741], Loss: 2.1758\n",
      "Epoch [28/100], Step [200/741], Loss: 2.2786\n",
      "Epoch [28/100], Step [300/741], Loss: 2.3151\n",
      "Epoch [28/100], Step [400/741], Loss: 2.2644\n",
      "Epoch [28/100], Step [500/741], Loss: 2.3644\n",
      "Epoch [28/100], Step [600/741], Loss: 2.3268\n",
      "Epoch [28/100], Step [700/741], Loss: 2.3892\n",
      "Epoch [29/100], Step [100/741], Loss: 2.3331\n",
      "Epoch [29/100], Step [200/741], Loss: 2.3383\n",
      "Epoch [29/100], Step [300/741], Loss: 2.3397\n",
      "Epoch [29/100], Step [400/741], Loss: 2.2835\n",
      "Epoch [29/100], Step [500/741], Loss: 2.2813\n",
      "Epoch [29/100], Step [600/741], Loss: 2.3576\n",
      "Epoch [29/100], Step [700/741], Loss: 2.3515\n",
      "Epoch [30/100], Step [100/741], Loss: 2.2398\n",
      "Epoch [30/100], Step [200/741], Loss: 2.2427\n",
      "Epoch [30/100], Step [300/741], Loss: 2.3141\n",
      "Epoch [30/100], Step [400/741], Loss: 2.3242\n",
      "Epoch [30/100], Step [500/741], Loss: 2.3359\n",
      "Epoch [30/100], Step [600/741], Loss: 2.2354\n",
      "Epoch [30/100], Step [700/741], Loss: 2.3068\n",
      "Epoch [31/100], Step [100/741], Loss: 2.3008\n",
      "Epoch [31/100], Step [200/741], Loss: 2.3573\n",
      "Epoch [31/100], Step [300/741], Loss: 2.2842\n",
      "Epoch [31/100], Step [400/741], Loss: 2.3275\n",
      "Epoch [31/100], Step [500/741], Loss: 2.3553\n",
      "Epoch [31/100], Step [600/741], Loss: 2.2778\n",
      "Epoch [31/100], Step [700/741], Loss: 2.3029\n",
      "Epoch [32/100], Step [100/741], Loss: 2.2643\n",
      "Epoch [32/100], Step [200/741], Loss: 2.3570\n",
      "Epoch [32/100], Step [300/741], Loss: 2.3147\n",
      "Epoch [32/100], Step [400/741], Loss: 2.3061\n",
      "Epoch [32/100], Step [500/741], Loss: 2.2250\n",
      "Epoch [32/100], Step [600/741], Loss: 2.3309\n",
      "Epoch [32/100], Step [700/741], Loss: 2.3360\n",
      "Epoch [33/100], Step [100/741], Loss: 2.3396\n",
      "Epoch [33/100], Step [200/741], Loss: 2.3106\n",
      "Epoch [33/100], Step [300/741], Loss: 2.2811\n",
      "Epoch [33/100], Step [400/741], Loss: 2.2277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     35\u001b[0m     n_total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     38\u001b[0m         out, _ \u001b[38;5;241m=\u001b[39m rnn(torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m     39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(out, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcqtlist[idx, :])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabellist[idx]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x, y]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Reference:\n",
    "https://www.youtube.com/watch?v=0_PgWWmauHk\n",
    "https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "'''\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.init_hidden()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size).to(device)\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    n_total_steps = len(train_loader)\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        out, _ = rnn(torch.Tensor(x))\n",
    "        loss = loss_fn(out, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "def predict(input_line):\n",
    "    print(f\"\\n> {input_line}\")\n",
    "    with torch.no_grad():\n",
    "        line_tensor = line_to_tensor(input_line)\n",
    "        \n",
    "        hidden = rnn.init_hidden()\n",
    "    \n",
    "        for i in range(line_tensor.size()[0]):\n",
    "            output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "        guess = category_from_output(output)\n",
    "        print(guess)\n",
    "\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"Input:\")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "    \n",
    "    predict(sentence)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d45fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/741], Loss: 2.6778\n",
      "Epoch [1/100], Step [200/741], Loss: 2.7518\n",
      "Epoch [1/100], Step [300/741], Loss: 2.8422\n",
      "Epoch [1/100], Step [400/741], Loss: 2.7264\n",
      "Epoch [1/100], Step [500/741], Loss: 2.7881\n",
      "Epoch [1/100], Step [600/741], Loss: 2.7572\n",
      "Epoch [1/100], Step [700/741], Loss: 2.7296\n",
      "Epoch [2/100], Step [100/741], Loss: 2.6835\n",
      "Epoch [2/100], Step [200/741], Loss: 2.7235\n",
      "Epoch [2/100], Step [300/741], Loss: 2.7495\n",
      "Epoch [2/100], Step [400/741], Loss: 2.5769\n",
      "Epoch [2/100], Step [500/741], Loss: 2.6274\n",
      "Epoch [2/100], Step [600/741], Loss: 2.6194\n",
      "Epoch [2/100], Step [700/741], Loss: 2.6570\n",
      "Epoch [3/100], Step [100/741], Loss: 2.6212\n",
      "Epoch [3/100], Step [200/741], Loss: 2.6292\n",
      "Epoch [3/100], Step [300/741], Loss: 2.5656\n",
      "Epoch [3/100], Step [400/741], Loss: 2.5559\n",
      "Epoch [3/100], Step [500/741], Loss: 2.5776\n",
      "Epoch [3/100], Step [600/741], Loss: 2.5468\n",
      "Epoch [3/100], Step [700/741], Loss: 2.5978\n",
      "Epoch [4/100], Step [100/741], Loss: 2.6145\n",
      "Epoch [4/100], Step [200/741], Loss: 2.6662\n",
      "Epoch [4/100], Step [300/741], Loss: 2.5610\n",
      "Epoch [4/100], Step [400/741], Loss: 2.5435\n",
      "Epoch [4/100], Step [500/741], Loss: 2.5318\n",
      "Epoch [4/100], Step [600/741], Loss: 2.5279\n",
      "Epoch [4/100], Step [700/741], Loss: 2.5160\n",
      "Epoch [5/100], Step [100/741], Loss: 2.5655\n",
      "Epoch [5/100], Step [200/741], Loss: 2.4901\n",
      "Epoch [5/100], Step [300/741], Loss: 2.4503\n",
      "Epoch [5/100], Step [400/741], Loss: 2.4687\n",
      "Epoch [5/100], Step [500/741], Loss: 2.4367\n",
      "Epoch [5/100], Step [600/741], Loss: 2.5261\n",
      "Epoch [5/100], Step [700/741], Loss: 2.4334\n",
      "Epoch [6/100], Step [100/741], Loss: 2.4349\n",
      "Epoch [6/100], Step [200/741], Loss: 2.4791\n",
      "Epoch [6/100], Step [300/741], Loss: 2.5228\n",
      "Epoch [6/100], Step [400/741], Loss: 2.4962\n",
      "Epoch [6/100], Step [500/741], Loss: 2.4806\n",
      "Epoch [6/100], Step [600/741], Loss: 2.3680\n",
      "Epoch [6/100], Step [700/741], Loss: 2.3316\n",
      "Epoch [7/100], Step [100/741], Loss: 2.4629\n",
      "Epoch [7/100], Step [200/741], Loss: 2.4616\n",
      "Epoch [7/100], Step [300/741], Loss: 2.3794\n",
      "Epoch [7/100], Step [400/741], Loss: 2.5146\n",
      "Epoch [7/100], Step [500/741], Loss: 2.4240\n",
      "Epoch [7/100], Step [600/741], Loss: 2.4013\n",
      "Epoch [7/100], Step [700/741], Loss: 2.5082\n",
      "Epoch [8/100], Step [100/741], Loss: 2.4439\n",
      "Epoch [8/100], Step [200/741], Loss: 2.3966\n",
      "Epoch [8/100], Step [300/741], Loss: 2.4641\n",
      "Epoch [8/100], Step [400/741], Loss: 2.3697\n",
      "Epoch [8/100], Step [500/741], Loss: 2.3628\n",
      "Epoch [8/100], Step [600/741], Loss: 2.3944\n",
      "Epoch [8/100], Step [700/741], Loss: 2.4973\n",
      "Epoch [9/100], Step [100/741], Loss: 2.3885\n",
      "Epoch [9/100], Step [200/741], Loss: 2.4199\n",
      "Epoch [9/100], Step [300/741], Loss: 2.3979\n",
      "Epoch [9/100], Step [400/741], Loss: 2.3798\n",
      "Epoch [9/100], Step [500/741], Loss: 2.3914\n",
      "Epoch [9/100], Step [600/741], Loss: 2.3713\n",
      "Epoch [9/100], Step [700/741], Loss: 2.4129\n",
      "Epoch [10/100], Step [100/741], Loss: 2.3186\n",
      "Epoch [10/100], Step [200/741], Loss: 2.4440\n",
      "Epoch [10/100], Step [300/741], Loss: 2.4137\n",
      "Epoch [10/100], Step [400/741], Loss: 2.4196\n",
      "Epoch [10/100], Step [500/741], Loss: 2.4088\n",
      "Epoch [10/100], Step [600/741], Loss: 2.3987\n",
      "Epoch [10/100], Step [700/741], Loss: 2.4468\n",
      "Epoch [11/100], Step [100/741], Loss: 2.3101\n",
      "Epoch [11/100], Step [200/741], Loss: 2.3534\n",
      "Epoch [11/100], Step [300/741], Loss: 2.4035\n",
      "Epoch [11/100], Step [400/741], Loss: 2.3917\n",
      "Epoch [11/100], Step [500/741], Loss: 2.3787\n",
      "Epoch [11/100], Step [600/741], Loss: 2.4404\n",
      "Epoch [11/100], Step [700/741], Loss: 2.4413\n",
      "Epoch [12/100], Step [100/741], Loss: 2.3217\n",
      "Epoch [12/100], Step [200/741], Loss: 2.3236\n",
      "Epoch [12/100], Step [300/741], Loss: 2.3946\n",
      "Epoch [12/100], Step [400/741], Loss: 2.3191\n",
      "Epoch [12/100], Step [500/741], Loss: 2.3058\n",
      "Epoch [12/100], Step [600/741], Loss: 2.3060\n",
      "Epoch [12/100], Step [700/741], Loss: 2.3835\n",
      "Epoch [13/100], Step [100/741], Loss: 2.3938\n",
      "Epoch [13/100], Step [200/741], Loss: 2.2974\n",
      "Epoch [13/100], Step [300/741], Loss: 2.3721\n",
      "Epoch [13/100], Step [400/741], Loss: 2.3737\n",
      "Epoch [13/100], Step [500/741], Loss: 2.2604\n",
      "Epoch [13/100], Step [600/741], Loss: 2.4057\n",
      "Epoch [13/100], Step [700/741], Loss: 2.4169\n",
      "Epoch [14/100], Step [100/741], Loss: 2.3387\n",
      "Epoch [14/100], Step [200/741], Loss: 2.4148\n",
      "Epoch [14/100], Step [300/741], Loss: 2.3209\n",
      "Epoch [14/100], Step [400/741], Loss: 2.3715\n",
      "Epoch [14/100], Step [500/741], Loss: 2.2441\n",
      "Epoch [14/100], Step [600/741], Loss: 2.4321\n",
      "Epoch [14/100], Step [700/741], Loss: 2.3368\n",
      "Epoch [15/100], Step [100/741], Loss: 2.3307\n",
      "Epoch [15/100], Step [200/741], Loss: 2.3776\n",
      "Epoch [15/100], Step [300/741], Loss: 2.3602\n",
      "Epoch [15/100], Step [400/741], Loss: 2.3283\n",
      "Epoch [15/100], Step [500/741], Loss: 2.3239\n",
      "Epoch [15/100], Step [600/741], Loss: 2.3318\n",
      "Epoch [15/100], Step [700/741], Loss: 2.3449\n",
      "Epoch [16/100], Step [100/741], Loss: 2.2961\n",
      "Epoch [16/100], Step [200/741], Loss: 2.3093\n",
      "Epoch [16/100], Step [300/741], Loss: 2.3627\n",
      "Epoch [16/100], Step [400/741], Loss: 2.3647\n",
      "Epoch [16/100], Step [500/741], Loss: 2.3954\n",
      "Epoch [16/100], Step [600/741], Loss: 2.4055\n",
      "Epoch [16/100], Step [700/741], Loss: 2.3302\n",
      "Epoch [17/100], Step [100/741], Loss: 2.3486\n",
      "Epoch [17/100], Step [200/741], Loss: 2.3017\n",
      "Epoch [17/100], Step [300/741], Loss: 2.3803\n",
      "Epoch [17/100], Step [400/741], Loss: 2.2817\n",
      "Epoch [17/100], Step [500/741], Loss: 2.4032\n",
      "Epoch [17/100], Step [600/741], Loss: 2.3184\n",
      "Epoch [17/100], Step [700/741], Loss: 2.3855\n",
      "Epoch [18/100], Step [100/741], Loss: 2.3380\n",
      "Epoch [18/100], Step [200/741], Loss: 2.2409\n",
      "Epoch [18/100], Step [300/741], Loss: 2.3315\n",
      "Epoch [18/100], Step [400/741], Loss: 2.3533\n",
      "Epoch [18/100], Step [500/741], Loss: 2.4107\n",
      "Epoch [18/100], Step [600/741], Loss: 2.3858\n",
      "Epoch [18/100], Step [700/741], Loss: 2.3356\n",
      "Epoch [19/100], Step [100/741], Loss: 2.3290\n",
      "Epoch [19/100], Step [200/741], Loss: 2.3166\n",
      "Epoch [19/100], Step [300/741], Loss: 2.3347\n",
      "Epoch [19/100], Step [400/741], Loss: 2.3372\n",
      "Epoch [19/100], Step [500/741], Loss: 2.3362\n",
      "Epoch [19/100], Step [600/741], Loss: 2.3320\n",
      "Epoch [19/100], Step [700/741], Loss: 2.4077\n",
      "Epoch [20/100], Step [100/741], Loss: 2.3129\n",
      "Epoch [20/100], Step [200/741], Loss: 2.3912\n",
      "Epoch [20/100], Step [300/741], Loss: 2.2877\n",
      "Epoch [20/100], Step [400/741], Loss: 2.3104\n",
      "Epoch [20/100], Step [500/741], Loss: 2.2642\n",
      "Epoch [20/100], Step [600/741], Loss: 2.2950\n",
      "Epoch [20/100], Step [700/741], Loss: 2.3546\n",
      "Epoch [21/100], Step [100/741], Loss: 2.2810\n",
      "Epoch [21/100], Step [200/741], Loss: 2.2593\n",
      "Epoch [21/100], Step [300/741], Loss: 2.3276\n",
      "Epoch [21/100], Step [400/741], Loss: 2.3275\n",
      "Epoch [21/100], Step [500/741], Loss: 2.3340\n",
      "Epoch [21/100], Step [600/741], Loss: 2.2474\n",
      "Epoch [21/100], Step [700/741], Loss: 2.3653\n",
      "Epoch [22/100], Step [100/741], Loss: 2.3816\n",
      "Epoch [22/100], Step [200/741], Loss: 2.3078\n",
      "Epoch [22/100], Step [300/741], Loss: 2.2603\n",
      "Epoch [22/100], Step [400/741], Loss: 2.3480\n",
      "Epoch [22/100], Step [500/741], Loss: 2.2586\n",
      "Epoch [22/100], Step [600/741], Loss: 2.3668\n",
      "Epoch [22/100], Step [700/741], Loss: 2.3282\n",
      "Epoch [23/100], Step [100/741], Loss: 2.2920\n",
      "Epoch [23/100], Step [200/741], Loss: 2.3075\n",
      "Epoch [23/100], Step [300/741], Loss: 2.4322\n",
      "Epoch [23/100], Step [400/741], Loss: 2.3333\n",
      "Epoch [23/100], Step [500/741], Loss: 2.3790\n",
      "Epoch [23/100], Step [600/741], Loss: 2.3054\n",
      "Epoch [23/100], Step [700/741], Loss: 2.3888\n",
      "Epoch [24/100], Step [100/741], Loss: 2.3655\n",
      "Epoch [24/100], Step [200/741], Loss: 2.2562\n",
      "Epoch [24/100], Step [300/741], Loss: 2.3753\n",
      "Epoch [24/100], Step [400/741], Loss: 2.2904\n",
      "Epoch [24/100], Step [500/741], Loss: 2.3197\n",
      "Epoch [24/100], Step [600/741], Loss: 2.2828\n",
      "Epoch [24/100], Step [700/741], Loss: 2.3524\n",
      "Epoch [25/100], Step [100/741], Loss: 2.2312\n",
      "Epoch [25/100], Step [200/741], Loss: 2.3354\n",
      "Epoch [25/100], Step [300/741], Loss: 2.4009\n",
      "Epoch [25/100], Step [400/741], Loss: 2.2772\n",
      "Epoch [25/100], Step [500/741], Loss: 2.3192\n",
      "Epoch [25/100], Step [600/741], Loss: 2.3096\n",
      "Epoch [25/100], Step [700/741], Loss: 2.2730\n",
      "Epoch [26/100], Step [100/741], Loss: 2.2657\n",
      "Epoch [26/100], Step [200/741], Loss: 2.2966\n",
      "Epoch [26/100], Step [300/741], Loss: 2.3634\n",
      "Epoch [26/100], Step [400/741], Loss: 2.3261\n",
      "Epoch [26/100], Step [500/741], Loss: 2.2524\n",
      "Epoch [26/100], Step [600/741], Loss: 2.3190\n",
      "Epoch [26/100], Step [700/741], Loss: 2.3291\n",
      "Epoch [27/100], Step [100/741], Loss: 2.2907\n",
      "Epoch [27/100], Step [200/741], Loss: 2.2351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Step [300/741], Loss: 2.3378\n",
      "Epoch [27/100], Step [400/741], Loss: 2.3550\n",
      "Epoch [27/100], Step [500/741], Loss: 2.2282\n",
      "Epoch [27/100], Step [600/741], Loss: 2.3621\n",
      "Epoch [27/100], Step [700/741], Loss: 2.3460\n",
      "Epoch [28/100], Step [100/741], Loss: 2.3159\n",
      "Epoch [28/100], Step [200/741], Loss: 2.3212\n",
      "Epoch [28/100], Step [300/741], Loss: 2.3930\n",
      "Epoch [28/100], Step [400/741], Loss: 2.2344\n",
      "Epoch [28/100], Step [500/741], Loss: 2.3585\n",
      "Epoch [28/100], Step [600/741], Loss: 2.1600\n",
      "Epoch [28/100], Step [700/741], Loss: 2.3013\n",
      "Epoch [29/100], Step [100/741], Loss: 2.3187\n",
      "Epoch [29/100], Step [200/741], Loss: 2.4267\n",
      "Epoch [29/100], Step [300/741], Loss: 2.2999\n",
      "Epoch [29/100], Step [400/741], Loss: 2.4115\n",
      "Epoch [29/100], Step [500/741], Loss: 2.3640\n",
      "Epoch [29/100], Step [600/741], Loss: 2.2440\n",
      "Epoch [29/100], Step [700/741], Loss: 2.3684\n",
      "Epoch [30/100], Step [100/741], Loss: 2.2136\n",
      "Epoch [30/100], Step [200/741], Loss: 2.2794\n",
      "Epoch [30/100], Step [300/741], Loss: 2.3339\n",
      "Epoch [30/100], Step [400/741], Loss: 2.2035\n",
      "Epoch [30/100], Step [500/741], Loss: 2.3083\n",
      "Epoch [30/100], Step [600/741], Loss: 2.2674\n",
      "Epoch [30/100], Step [700/741], Loss: 2.2217\n",
      "Epoch [31/100], Step [100/741], Loss: 2.2723\n",
      "Epoch [31/100], Step [200/741], Loss: 2.2794\n",
      "Epoch [31/100], Step [300/741], Loss: 2.2339\n",
      "Epoch [31/100], Step [400/741], Loss: 2.2912\n",
      "Epoch [31/100], Step [500/741], Loss: 2.3046\n",
      "Epoch [31/100], Step [600/741], Loss: 2.3217\n",
      "Epoch [31/100], Step [700/741], Loss: 2.3087\n",
      "Epoch [32/100], Step [100/741], Loss: 2.2855\n",
      "Epoch [32/100], Step [200/741], Loss: 2.2790\n",
      "Epoch [32/100], Step [300/741], Loss: 2.3557\n",
      "Epoch [32/100], Step [400/741], Loss: 2.3150\n",
      "Epoch [32/100], Step [500/741], Loss: 2.3611\n",
      "Epoch [32/100], Step [600/741], Loss: 2.3503\n",
      "Epoch [32/100], Step [700/741], Loss: 2.3768\n",
      "Epoch [33/100], Step [100/741], Loss: 2.2965\n",
      "Epoch [33/100], Step [200/741], Loss: 2.3136\n",
      "Epoch [33/100], Step [300/741], Loss: 2.3256\n",
      "Epoch [33/100], Step [400/741], Loss: 2.3233\n",
      "Epoch [33/100], Step [500/741], Loss: 2.2768\n",
      "Epoch [33/100], Step [600/741], Loss: 2.3140\n",
      "Epoch [33/100], Step [700/741], Loss: 2.4279\n",
      "Epoch [34/100], Step [100/741], Loss: 2.2315\n",
      "Epoch [34/100], Step [200/741], Loss: 2.2345\n",
      "Epoch [34/100], Step [300/741], Loss: 2.2296\n",
      "Epoch [34/100], Step [400/741], Loss: 2.2344\n",
      "Epoch [34/100], Step [500/741], Loss: 2.3989\n",
      "Epoch [34/100], Step [600/741], Loss: 2.4118\n",
      "Epoch [34/100], Step [700/741], Loss: 2.2941\n",
      "Epoch [35/100], Step [100/741], Loss: 2.3001\n",
      "Epoch [35/100], Step [200/741], Loss: 2.2527\n",
      "Epoch [35/100], Step [300/741], Loss: 2.2817\n",
      "Epoch [35/100], Step [400/741], Loss: 2.3946\n",
      "Epoch [35/100], Step [500/741], Loss: 2.3771\n",
      "Epoch [35/100], Step [600/741], Loss: 2.3444\n",
      "Epoch [35/100], Step [700/741], Loss: 2.2995\n",
      "Epoch [36/100], Step [100/741], Loss: 2.3065\n",
      "Epoch [36/100], Step [200/741], Loss: 2.2809\n",
      "Epoch [36/100], Step [300/741], Loss: 2.2112\n",
      "Epoch [36/100], Step [400/741], Loss: 2.4193\n",
      "Epoch [36/100], Step [500/741], Loss: 2.2877\n",
      "Epoch [36/100], Step [600/741], Loss: 2.4013\n",
      "Epoch [36/100], Step [700/741], Loss: 2.3557\n",
      "Epoch [37/100], Step [100/741], Loss: 2.3225\n",
      "Epoch [37/100], Step [200/741], Loss: 2.3410\n",
      "Epoch [37/100], Step [300/741], Loss: 2.3857\n",
      "Epoch [37/100], Step [400/741], Loss: 2.1737\n",
      "Epoch [37/100], Step [500/741], Loss: 2.1808\n",
      "Epoch [37/100], Step [600/741], Loss: 2.3027\n",
      "Epoch [37/100], Step [700/741], Loss: 2.2929\n",
      "Epoch [38/100], Step [100/741], Loss: 2.2872\n",
      "Epoch [38/100], Step [200/741], Loss: 2.3775\n",
      "Epoch [38/100], Step [300/741], Loss: 2.2111\n",
      "Epoch [38/100], Step [400/741], Loss: 2.2488\n",
      "Epoch [38/100], Step [500/741], Loss: 2.4378\n",
      "Epoch [38/100], Step [600/741], Loss: 2.3161\n",
      "Epoch [38/100], Step [700/741], Loss: 2.3209\n",
      "Epoch [39/100], Step [100/741], Loss: 2.3421\n",
      "Epoch [39/100], Step [200/741], Loss: 2.2728\n",
      "Epoch [39/100], Step [300/741], Loss: 2.2793\n",
      "Epoch [39/100], Step [400/741], Loss: 2.3010\n",
      "Epoch [39/100], Step [500/741], Loss: 2.3608\n",
      "Epoch [39/100], Step [600/741], Loss: 2.2782\n",
      "Epoch [39/100], Step [700/741], Loss: 2.3614\n",
      "Epoch [40/100], Step [100/741], Loss: 2.2474\n",
      "Epoch [40/100], Step [200/741], Loss: 2.2823\n",
      "Epoch [40/100], Step [300/741], Loss: 2.3874\n",
      "Epoch [40/100], Step [400/741], Loss: 2.3607\n",
      "Epoch [40/100], Step [500/741], Loss: 2.3921\n",
      "Epoch [40/100], Step [600/741], Loss: 2.2571\n",
      "Epoch [40/100], Step [700/741], Loss: 2.3274\n",
      "Epoch [41/100], Step [100/741], Loss: 2.3436\n",
      "Epoch [41/100], Step [200/741], Loss: 2.3297\n",
      "Epoch [41/100], Step [300/741], Loss: 2.3407\n",
      "Epoch [41/100], Step [400/741], Loss: 2.3586\n",
      "Epoch [41/100], Step [500/741], Loss: 2.2943\n",
      "Epoch [41/100], Step [600/741], Loss: 2.3469\n",
      "Epoch [41/100], Step [700/741], Loss: 2.2089\n",
      "Epoch [42/100], Step [100/741], Loss: 2.1991\n",
      "Epoch [42/100], Step [200/741], Loss: 2.2663\n",
      "Epoch [42/100], Step [300/741], Loss: 2.3088\n",
      "Epoch [42/100], Step [400/741], Loss: 2.3304\n",
      "Epoch [42/100], Step [500/741], Loss: 2.3204\n",
      "Epoch [42/100], Step [600/741], Loss: 2.1861\n",
      "Epoch [42/100], Step [700/741], Loss: 2.2107\n",
      "Epoch [43/100], Step [100/741], Loss: 2.2630\n",
      "Epoch [43/100], Step [200/741], Loss: 2.2442\n",
      "Epoch [43/100], Step [300/741], Loss: 2.3196\n",
      "Epoch [43/100], Step [400/741], Loss: 2.2214\n",
      "Epoch [43/100], Step [500/741], Loss: 2.2928\n",
      "Epoch [43/100], Step [600/741], Loss: 2.3578\n",
      "Epoch [43/100], Step [700/741], Loss: 2.3353\n",
      "Epoch [44/100], Step [100/741], Loss: 2.2589\n",
      "Epoch [44/100], Step [200/741], Loss: 2.2482\n",
      "Epoch [44/100], Step [300/741], Loss: 2.3438\n",
      "Epoch [44/100], Step [400/741], Loss: 2.2483\n",
      "Epoch [44/100], Step [500/741], Loss: 2.3417\n",
      "Epoch [44/100], Step [600/741], Loss: 2.2663\n",
      "Epoch [44/100], Step [700/741], Loss: 2.3796\n",
      "Epoch [45/100], Step [100/741], Loss: 2.2286\n",
      "Epoch [45/100], Step [200/741], Loss: 2.2345\n",
      "Epoch [45/100], Step [300/741], Loss: 2.2529\n",
      "Epoch [45/100], Step [400/741], Loss: 2.2906\n",
      "Epoch [45/100], Step [500/741], Loss: 2.2784\n",
      "Epoch [45/100], Step [600/741], Loss: 2.2941\n",
      "Epoch [45/100], Step [700/741], Loss: 2.3147\n",
      "Epoch [46/100], Step [100/741], Loss: 2.2036\n",
      "Epoch [46/100], Step [200/741], Loss: 2.2471\n",
      "Epoch [46/100], Step [300/741], Loss: 2.2775\n",
      "Epoch [46/100], Step [400/741], Loss: 2.2770\n",
      "Epoch [46/100], Step [500/741], Loss: 2.3587\n",
      "Epoch [46/100], Step [600/741], Loss: 2.4338\n",
      "Epoch [46/100], Step [700/741], Loss: 2.3402\n",
      "Epoch [47/100], Step [100/741], Loss: 2.2935\n",
      "Epoch [47/100], Step [200/741], Loss: 2.3383\n",
      "Epoch [47/100], Step [300/741], Loss: 2.2732\n",
      "Epoch [47/100], Step [400/741], Loss: 2.3086\n",
      "Epoch [47/100], Step [500/741], Loss: 2.2542\n",
      "Epoch [47/100], Step [600/741], Loss: 2.2465\n",
      "Epoch [47/100], Step [700/741], Loss: 2.2833\n",
      "Epoch [48/100], Step [100/741], Loss: 2.2909\n",
      "Epoch [48/100], Step [200/741], Loss: 2.3208\n",
      "Epoch [48/100], Step [300/741], Loss: 2.3316\n",
      "Epoch [48/100], Step [400/741], Loss: 2.2243\n",
      "Epoch [48/100], Step [500/741], Loss: 2.2926\n",
      "Epoch [48/100], Step [600/741], Loss: 2.2545\n",
      "Epoch [48/100], Step [700/741], Loss: 2.2544\n",
      "Epoch [49/100], Step [100/741], Loss: 2.2436\n",
      "Epoch [49/100], Step [200/741], Loss: 2.2834\n",
      "Epoch [49/100], Step [300/741], Loss: 2.1900\n",
      "Epoch [49/100], Step [400/741], Loss: 2.2758\n",
      "Epoch [49/100], Step [500/741], Loss: 2.3327\n",
      "Epoch [49/100], Step [600/741], Loss: 2.3271\n",
      "Epoch [49/100], Step [700/741], Loss: 2.3711\n",
      "Epoch [50/100], Step [100/741], Loss: 2.2649\n",
      "Epoch [50/100], Step [200/741], Loss: 2.3476\n",
      "Epoch [50/100], Step [300/741], Loss: 2.2753\n",
      "Epoch [50/100], Step [400/741], Loss: 2.2659\n",
      "Epoch [50/100], Step [500/741], Loss: 2.2992\n",
      "Epoch [50/100], Step [600/741], Loss: 2.2995\n",
      "Epoch [50/100], Step [700/741], Loss: 2.2833\n",
      "Epoch [51/100], Step [100/741], Loss: 2.2811\n",
      "Epoch [51/100], Step [200/741], Loss: 2.3516\n",
      "Epoch [51/100], Step [300/741], Loss: 2.3410\n",
      "Epoch [51/100], Step [400/741], Loss: 2.2605\n",
      "Epoch [51/100], Step [500/741], Loss: 2.3126\n",
      "Epoch [51/100], Step [600/741], Loss: 2.3462\n",
      "Epoch [51/100], Step [700/741], Loss: 2.3117\n",
      "Epoch [52/100], Step [100/741], Loss: 2.2536\n",
      "Epoch [52/100], Step [200/741], Loss: 2.2552\n",
      "Epoch [52/100], Step [300/741], Loss: 2.2657\n",
      "Epoch [52/100], Step [400/741], Loss: 2.2923\n",
      "Epoch [52/100], Step [500/741], Loss: 2.2067\n",
      "Epoch [52/100], Step [600/741], Loss: 2.2693\n",
      "Epoch [52/100], Step [700/741], Loss: 2.2783\n",
      "Epoch [53/100], Step [100/741], Loss: 2.1608\n",
      "Epoch [53/100], Step [200/741], Loss: 2.2044\n",
      "Epoch [53/100], Step [300/741], Loss: 2.3205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [400/741], Loss: 2.2937\n",
      "Epoch [53/100], Step [500/741], Loss: 2.4034\n",
      "Epoch [53/100], Step [600/741], Loss: 2.4441\n",
      "Epoch [53/100], Step [700/741], Loss: 2.2569\n",
      "Epoch [54/100], Step [100/741], Loss: 2.2175\n",
      "Epoch [54/100], Step [200/741], Loss: 2.2150\n",
      "Epoch [54/100], Step [300/741], Loss: 2.3139\n",
      "Epoch [54/100], Step [400/741], Loss: 2.3023\n",
      "Epoch [54/100], Step [500/741], Loss: 2.2957\n",
      "Epoch [54/100], Step [600/741], Loss: 2.2646\n",
      "Epoch [54/100], Step [700/741], Loss: 2.2902\n",
      "Epoch [55/100], Step [100/741], Loss: 2.3517\n",
      "Epoch [55/100], Step [200/741], Loss: 2.2951\n",
      "Epoch [55/100], Step [300/741], Loss: 2.2828\n",
      "Epoch [55/100], Step [400/741], Loss: 2.3077\n",
      "Epoch [55/100], Step [500/741], Loss: 2.2905\n",
      "Epoch [55/100], Step [600/741], Loss: 2.3460\n",
      "Epoch [55/100], Step [700/741], Loss: 2.3280\n",
      "Epoch [56/100], Step [100/741], Loss: 2.3282\n",
      "Epoch [56/100], Step [200/741], Loss: 2.3187\n",
      "Epoch [56/100], Step [300/741], Loss: 2.2889\n",
      "Epoch [56/100], Step [400/741], Loss: 2.3656\n",
      "Epoch [56/100], Step [500/741], Loss: 2.3409\n",
      "Epoch [56/100], Step [600/741], Loss: 2.2694\n",
      "Epoch [56/100], Step [700/741], Loss: 2.3285\n",
      "Epoch [57/100], Step [100/741], Loss: 2.3229\n",
      "Epoch [57/100], Step [200/741], Loss: 2.2048\n",
      "Epoch [57/100], Step [300/741], Loss: 2.2287\n",
      "Epoch [57/100], Step [400/741], Loss: 2.2030\n",
      "Epoch [57/100], Step [500/741], Loss: 2.3394\n",
      "Epoch [57/100], Step [600/741], Loss: 2.3187\n",
      "Epoch [57/100], Step [700/741], Loss: 2.2282\n",
      "Epoch [58/100], Step [100/741], Loss: 2.3454\n",
      "Epoch [58/100], Step [200/741], Loss: 2.3567\n",
      "Epoch [58/100], Step [300/741], Loss: 2.2748\n",
      "Epoch [58/100], Step [400/741], Loss: 2.2297\n",
      "Epoch [58/100], Step [500/741], Loss: 2.2785\n",
      "Epoch [58/100], Step [600/741], Loss: 2.3159\n",
      "Epoch [58/100], Step [700/741], Loss: 2.2902\n",
      "Epoch [59/100], Step [100/741], Loss: 2.3391\n",
      "Epoch [59/100], Step [200/741], Loss: 2.2882\n",
      "Epoch [59/100], Step [300/741], Loss: 2.2901\n",
      "Epoch [59/100], Step [400/741], Loss: 2.2014\n",
      "Epoch [59/100], Step [500/741], Loss: 2.2965\n",
      "Epoch [59/100], Step [600/741], Loss: 2.3966\n",
      "Epoch [59/100], Step [700/741], Loss: 2.2093\n",
      "Epoch [60/100], Step [100/741], Loss: 2.2343\n",
      "Epoch [60/100], Step [200/741], Loss: 2.2095\n",
      "Epoch [60/100], Step [300/741], Loss: 2.2836\n",
      "Epoch [60/100], Step [400/741], Loss: 2.3183\n",
      "Epoch [60/100], Step [500/741], Loss: 2.2607\n",
      "Epoch [60/100], Step [600/741], Loss: 2.2870\n",
      "Epoch [60/100], Step [700/741], Loss: 2.3213\n",
      "Epoch [61/100], Step [100/741], Loss: 2.3320\n",
      "Epoch [61/100], Step [200/741], Loss: 2.3350\n",
      "Epoch [61/100], Step [300/741], Loss: 2.3109\n",
      "Epoch [61/100], Step [400/741], Loss: 2.3177\n",
      "Epoch [61/100], Step [500/741], Loss: 2.3044\n",
      "Epoch [61/100], Step [600/741], Loss: 2.2211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     35\u001b[0m     n_total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     38\u001b[0m         out, _ \u001b[38;5;241m=\u001b[39m rnn(torch\u001b[38;5;241m.\u001b[39mTensor(x))\n\u001b[1;32m     39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(out, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcqtlist[idx, :])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabellist[idx]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x, y]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Ref: \n",
    "https://www.youtube.com/watch?v=0_PgWWmauHk\n",
    "https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/'''\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.init_hidden()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size).to(device)\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    n_total_steps = len(train_loader)\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        out, _ = rnn(torch.Tensor(x))\n",
    "        loss = loss_fn(out, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "def predict(input_line):\n",
    "    print(f\"\\n> {input_line}\")\n",
    "    with torch.no_grad():\n",
    "        line_tensor = line_to_tensor(input_line)\n",
    "        \n",
    "        hidden = rnn.init_hidden()\n",
    "    \n",
    "        for i in range(line_tensor.size()[0]):\n",
    "            output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "        guess = category_from_output(output)\n",
    "        print(guess)\n",
    "\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"Input:\")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "    \n",
    "    predict(sentence)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1671f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebb6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnn\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(138, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 25),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23904d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b41863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d3408c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, optimizer):\n",
    "    size = len(train_loader)\n",
    "    for e in range(num_epochs):\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                loss, current = loss.item(), (i + 1)\n",
    "                print(f\"epoch {e}: loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a87addf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss: 3.230472  [    1/  741]\n",
      "epoch 0: loss: 2.732932  [  101/  741]\n",
      "epoch 0: loss: 2.713971  [  201/  741]\n",
      "epoch 0: loss: 2.658298  [  301/  741]\n",
      "epoch 0: loss: 2.683450  [  401/  741]\n",
      "epoch 0: loss: 2.646077  [  501/  741]\n",
      "epoch 0: loss: 2.651324  [  601/  741]\n",
      "epoch 0: loss: 2.700296  [  701/  741]\n",
      "epoch 1: loss: 2.579834  [    1/  741]\n",
      "epoch 1: loss: 2.546856  [  101/  741]\n",
      "epoch 1: loss: 2.573468  [  201/  741]\n",
      "epoch 1: loss: 2.590904  [  301/  741]\n",
      "epoch 1: loss: 2.547320  [  401/  741]\n",
      "epoch 1: loss: 2.541674  [  501/  741]\n",
      "epoch 1: loss: 2.533453  [  601/  741]\n",
      "epoch 1: loss: 2.509900  [  701/  741]\n",
      "epoch 2: loss: 2.495333  [    1/  741]\n",
      "epoch 2: loss: 2.447051  [  101/  741]\n",
      "epoch 2: loss: 2.528401  [  201/  741]\n",
      "epoch 2: loss: 2.520875  [  301/  741]\n",
      "epoch 2: loss: 2.409639  [  401/  741]\n",
      "epoch 2: loss: 2.514967  [  501/  741]\n",
      "epoch 2: loss: 2.524416  [  601/  741]\n",
      "epoch 2: loss: 2.438348  [  701/  741]\n",
      "epoch 3: loss: 2.381194  [    1/  741]\n",
      "epoch 3: loss: 2.379784  [  101/  741]\n",
      "epoch 3: loss: 2.425029  [  201/  741]\n",
      "epoch 3: loss: 2.510900  [  301/  741]\n",
      "epoch 3: loss: 2.497023  [  401/  741]\n",
      "epoch 3: loss: 2.522669  [  501/  741]\n",
      "epoch 3: loss: 2.420241  [  601/  741]\n",
      "epoch 3: loss: 2.348634  [  701/  741]\n",
      "epoch 4: loss: 2.413198  [    1/  741]\n",
      "epoch 4: loss: 2.454000  [  101/  741]\n",
      "epoch 4: loss: 2.437550  [  201/  741]\n",
      "epoch 4: loss: 2.388178  [  301/  741]\n",
      "epoch 4: loss: 2.427673  [  401/  741]\n",
      "epoch 4: loss: 2.477762  [  501/  741]\n",
      "epoch 4: loss: 2.408300  [  601/  741]\n",
      "epoch 4: loss: 2.435176  [  701/  741]\n",
      "epoch 5: loss: 2.333358  [    1/  741]\n",
      "epoch 5: loss: 2.288090  [  101/  741]\n",
      "epoch 5: loss: 2.248113  [  201/  741]\n",
      "epoch 5: loss: 2.392828  [  301/  741]\n",
      "epoch 5: loss: 2.367490  [  401/  741]\n",
      "epoch 5: loss: 2.440014  [  501/  741]\n",
      "epoch 5: loss: 2.306540  [  601/  741]\n",
      "epoch 5: loss: 2.289599  [  701/  741]\n",
      "epoch 6: loss: 2.408686  [    1/  741]\n",
      "epoch 6: loss: 2.420183  [  101/  741]\n",
      "epoch 6: loss: 2.397286  [  201/  741]\n",
      "epoch 6: loss: 2.390064  [  301/  741]\n",
      "epoch 6: loss: 2.372742  [  401/  741]\n",
      "epoch 6: loss: 2.349994  [  501/  741]\n",
      "epoch 6: loss: 2.428125  [  601/  741]\n",
      "epoch 6: loss: 2.388336  [  701/  741]\n",
      "epoch 7: loss: 2.353753  [    1/  741]\n",
      "epoch 7: loss: 2.320891  [  101/  741]\n",
      "epoch 7: loss: 2.399929  [  201/  741]\n",
      "epoch 7: loss: 2.248333  [  301/  741]\n",
      "epoch 7: loss: 2.362372  [  401/  741]\n",
      "epoch 7: loss: 2.377977  [  501/  741]\n",
      "epoch 7: loss: 2.339430  [  601/  741]\n",
      "epoch 7: loss: 2.261704  [  701/  741]\n",
      "epoch 8: loss: 2.402190  [    1/  741]\n",
      "epoch 8: loss: 2.278475  [  101/  741]\n",
      "epoch 8: loss: 2.389565  [  201/  741]\n",
      "epoch 8: loss: 2.257210  [  301/  741]\n",
      "epoch 8: loss: 2.317547  [  401/  741]\n",
      "epoch 8: loss: 2.258207  [  501/  741]\n",
      "epoch 8: loss: 2.366419  [  601/  741]\n",
      "epoch 8: loss: 2.403996  [  701/  741]\n",
      "epoch 9: loss: 2.351204  [    1/  741]\n",
      "epoch 9: loss: 2.256943  [  101/  741]\n",
      "epoch 9: loss: 2.362192  [  201/  741]\n",
      "epoch 9: loss: 2.350719  [  301/  741]\n",
      "epoch 9: loss: 2.306116  [  401/  741]\n",
      "epoch 9: loss: 2.220139  [  501/  741]\n",
      "epoch 9: loss: 2.331660  [  601/  741]\n",
      "epoch 9: loss: 2.316066  [  701/  741]\n",
      "epoch 10: loss: 2.292070  [    1/  741]\n",
      "epoch 10: loss: 2.150738  [  101/  741]\n",
      "epoch 10: loss: 2.295455  [  201/  741]\n",
      "epoch 10: loss: 2.291528  [  301/  741]\n",
      "epoch 10: loss: 2.355534  [  401/  741]\n",
      "epoch 10: loss: 2.291429  [  501/  741]\n",
      "epoch 10: loss: 2.299018  [  601/  741]\n",
      "epoch 10: loss: 2.303992  [  701/  741]\n",
      "epoch 11: loss: 2.233339  [    1/  741]\n",
      "epoch 11: loss: 2.286358  [  101/  741]\n",
      "epoch 11: loss: 2.203644  [  201/  741]\n",
      "epoch 11: loss: 2.268903  [  301/  741]\n",
      "epoch 11: loss: 2.298298  [  401/  741]\n",
      "epoch 11: loss: 2.409267  [  501/  741]\n",
      "epoch 11: loss: 2.321014  [  601/  741]\n",
      "epoch 11: loss: 2.188072  [  701/  741]\n",
      "epoch 12: loss: 2.236159  [    1/  741]\n",
      "epoch 12: loss: 2.275841  [  101/  741]\n",
      "epoch 12: loss: 2.279665  [  201/  741]\n",
      "epoch 12: loss: 2.200637  [  301/  741]\n",
      "epoch 12: loss: 2.295875  [  401/  741]\n",
      "epoch 12: loss: 2.355847  [  501/  741]\n",
      "epoch 12: loss: 2.303266  [  601/  741]\n",
      "epoch 12: loss: 2.295249  [  701/  741]\n",
      "epoch 13: loss: 2.267183  [    1/  741]\n",
      "epoch 13: loss: 2.323336  [  101/  741]\n",
      "epoch 13: loss: 2.302753  [  201/  741]\n",
      "epoch 13: loss: 2.226557  [  301/  741]\n",
      "epoch 13: loss: 2.322428  [  401/  741]\n",
      "epoch 13: loss: 2.269902  [  501/  741]\n",
      "epoch 13: loss: 2.170399  [  601/  741]\n",
      "epoch 13: loss: 2.308195  [  701/  741]\n",
      "epoch 14: loss: 2.346829  [    1/  741]\n",
      "epoch 14: loss: 2.221880  [  101/  741]\n",
      "epoch 14: loss: 2.311048  [  201/  741]\n",
      "epoch 14: loss: 2.236497  [  301/  741]\n",
      "epoch 14: loss: 2.247247  [  401/  741]\n",
      "epoch 14: loss: 2.327448  [  501/  741]\n",
      "epoch 14: loss: 2.220667  [  601/  741]\n",
      "epoch 14: loss: 2.208730  [  701/  741]\n",
      "epoch 15: loss: 2.266663  [    1/  741]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m      8\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcqtlist[idx, :])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabellist[idx]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x, y]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5270d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = test_dataset[0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df07cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4344, -0.9752, -0.7345, -0.8160,  1.1610, -0.5414, -1.1247, -2.4593,\n",
      "         0.9745, -0.2917, -0.3639, -1.0234, -0.1265, -0.6864,  1.0501, -1.9562,\n",
      "        -0.1258, -2.5222,  1.3533, -0.0928, -0.1041, -1.4251,  0.3371, -0.3832,\n",
      "        -0.2196], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "    print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bff8e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:maj'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonIdxToChord(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b130177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test image: 20.85%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        out = model(X)\n",
    "        predicted = torch.argmax(out.data, 1)\n",
    "        n_samples += y.size(0)\n",
    "        label = torch.argmax(y, 1)\n",
    "        n_correct += (predicted == label).sum().item()\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test image: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b008fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
